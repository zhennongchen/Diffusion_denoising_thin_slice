{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/host/d/Github')\n",
    "import nibabel as nb\n",
    "import glob\n",
    "import os\n",
    "import glob\n",
    "import lpips\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "import Diffusion_denoising_thin_slice.functions_collection as ff\n",
    "import Diffusion_denoising_thin_slice.Build_lists.Build_list as Build_list\n",
    "import Diffusion_denoising_thin_slice.Data_processing as Data_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sheet =  Build_list.Build_thinsliceCT(os.path.join('/host/d/Data/brain_CT/Patient_lists/fixedCT_static_simulation_train_test_gaussian_xjtlu.xlsx'))\n",
    "_,patient_id_list,patient_subid_list,random_num_list, condition_list, x0_list = build_sheet.__build__(batch_list = [5]) \n",
    "n = ff.get_X_numbers_in_interval(total_number = patient_id_list.shape[0],start_number = 0,end_number = 1, interval = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae_with_ref_window(img, ref, vmin, vmax):\n",
    "    maes = []\n",
    "    for slice_num in range(0, img.shape[-1]):\n",
    "        slice_img = img[:,:,slice_num]\n",
    "        slice_ref = ref[:,:,slice_num]\n",
    "        mask = np.where((slice_ref >= vmin) & (slice_ref <= vmax), 1, 0)\n",
    "        mae = np.sum(np.abs(slice_img - slice_ref) * mask) / np.sum(mask)\n",
    "        maes.append(mae)\n",
    "\n",
    "    return np.mean(maes), np.std(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ssim_with_ref_window(img, ref, vmin, vmax):\n",
    "\n",
    "    ssims = []\n",
    "    for slice_num in range(0, img.shape[-1]):\n",
    "        slice_img = img[:,:,slice_num]\n",
    "        slice_ref = ref[:,:,slice_num]\n",
    "        mask = np.where((slice_ref >= vmin) & (slice_ref <= vmax), 1, 0)\n",
    "        _, ssim_map = structural_similarity(slice_img, slice_ref, data_range=vmax - vmin, full=True)\n",
    "        ssim = np.sum(ssim_map * mask) / np.sum(mask)\n",
    "        ssims.append(ssim)\n",
    "\n",
    "    return np.mean(ssims), np.std(ssims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lpips(imgs1, imgs2, vmin, vmax):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    loss_fn = lpips.LPIPS().to(device)\n",
    "    \n",
    "    lpipss = []\n",
    "    for slice_num in range(0, imgs1.shape[-1]):\n",
    "        slice1 = imgs1[:,:,slice_num]\n",
    "        slice2 = imgs2[:,:,slice_num]\n",
    "\n",
    "        slice1 = np.clip(slice1, vmin, vmax).astype(np.float32)\n",
    "        slice2 = np.clip(slice2, vmin, vmax).astype(np.float32)\n",
    "\n",
    "        slice1 = (slice1 - vmin) / (vmax - vmin) * 2 - 1\n",
    "        slice2 = (slice2 - vmin) / (vmax - vmin) * 2 - 1\n",
    "\n",
    "        slice1 = np.stack([slice1, slice1, slice1], axis=-1)\n",
    "        slice2 = np.stack([slice2, slice2, slice2], axis=-1)\n",
    "        # print('after stack, slice1 shape:', slice1.shape, ' slice2 shape:', slice2.shape)\n",
    "\n",
    "        slice1 = np.transpose(slice1, (2, 0, 1))[np.newaxis, ...]\n",
    "        slice2 = np.transpose(slice2, (2, 0, 1))[np.newaxis, ...]\n",
    "        # print('after transpose, slice1 shape:', slice1.shape, ' slice2 shape:', slice2.shape)\n",
    "\n",
    "        slice1 = torch.from_numpy(slice1).to(device)\n",
    "        slice2 = torch.from_numpy(slice2).to(device)\n",
    "\n",
    "        lpips_val = loss_fn(slice1, slice2)\n",
    "        lpipss.append(lpips_val.item())\n",
    "\n",
    "      \n",
    "\n",
    "    return np.mean(lpipss), np.std(lpipss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess noise2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00214841 0000455418 0\n",
      "mean: gt: 21.754015723196854  condition: 21.89032840789795  noise2noise: 19.03606891879365  noise2score: 15.506743506469727\n",
      "00105734 0000455323 0\n",
      "mean: gt: 18.486178371301225  condition: 18.97817980621338  noise2noise: 14.566930114135912  noise2score: 14.303804944152832\n",
      "00214867 0000455521 0\n",
      "mean: gt: 32.106963860106056  condition: 32.13290597808838  noise2noise: 30.043341023751566  noise2score: 25.375424999389647\n",
      "00214877 0000455524 0\n",
      "mean: gt: 31.27295554296927  condition: 31.359965008850097  noise2noise: 28.43605288487233  noise2score: 25.188118078308104\n",
      "00214836 0000455414 0\n",
      "mean: gt: 15.307165243522553  condition: 15.87778719329834  noise2noise: 11.581240975344958  noise2score: 9.508770465698243\n",
      "00154137 0000455529 0\n",
      "mean: gt: 32.78842260734797  condition: 32.843813273010255  noise2noise: 29.875630303859552  noise2score: 26.09790611022949\n",
      "00214901 0000455577 0\n",
      "mean: gt: 25.038895094831215  condition: 25.23733533203125  noise2noise: 21.86812425697546  noise2score: 20.032635048828126\n",
      "00174234 0000455725 0\n",
      "mean: gt: 33.013389452015005  condition: 33.0570856829834  noise2noise: 29.878002034123746  noise2score: 26.718518627624512\n",
      "00035838 0000455369 0\n",
      "mean: gt: 28.011231974495043  condition: 28.0446126272583  noise2noise: 25.451021493852803  noise2score: 22.12695564331055\n",
      "00019591 0000455445 0\n",
      "mean: gt: 26.20854599996504  condition: 26.382884604797365  noise2noise: 23.104259065508284  noise2score: 20.52938629394531\n",
      "00214792 0000455334 0\n",
      "mean: gt: 28.173691086568255  condition: 28.250673081970216  noise2noise: 23.93795570580166  noise2score: 22.621640180664063\n",
      "00010461 0000455845 0\n",
      "mean: gt: 19.004440949339866  condition: 19.510436942749024  noise2noise: 16.364205121292148  noise2score: 13.217095719299316\n",
      "00214931 0000455660 0\n",
      "mean: gt: 25.951439257488364  condition: 26.00678154083252  noise2noise: 24.511812689099038  noise2score: 20.2572171862793\n",
      "00010436 0000034479 0\n",
      "mean: gt: 38.85263744888841  condition: 38.872592513122555  noise2noise: 35.34338895587709  noise2score: 32.76991178771973\n",
      "00214878 0000455518 0\n",
      "mean: gt: 29.819331240794657  condition: 29.924660793762207  noise2noise: 27.324268715919217  noise2score: 24.160492078552245\n",
      "00014689 0000455416 0\n",
      "mean: gt: 25.355246732678207  condition: 25.449773285217287  noise2noise: 23.7514755617571  noise2score: 18.38119568939209\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(0,n.shape[0]):\n",
    "    patient_id = patient_id_list[n[i]]\n",
    "    patient_subid = patient_subid_list[n[i]]\n",
    "    random_n = random_num_list[n[i]]\n",
    "    print(patient_id, patient_subid, random_n)\n",
    "\n",
    "    # reference image\n",
    "    gt_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/gt_img.nii.gz')\n",
    "    gt_img = nb.load(gt_file).get_fdata()\n",
    "\n",
    "    # noisy image\n",
    "    condition_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/condition_img.nii.gz')\n",
    "    condition_img = nb.load(condition_file).get_fdata()\n",
    "\n",
    "    # noise2noise\n",
    "    noise2noise_file = os.path.join('/host/d/projects/denoising/models/noise2noise_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch78/pred_img.nii.gz')\n",
    "    noise2noise_img = nb.load(noise2noise_file).get_fdata() \n",
    "\n",
    "    # noise2score\n",
    "    noise2score_file = os.path.join('/host/d/projects/denoising/models/noise2score_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch100/pred_img.nii.gz')\n",
    "    noise2score_img = nb.load(noise2score_file).get_fdata()\n",
    "\n",
    "    ## calculate main\n",
    "    x,y = 256,256\n",
    "    mean_gt = np.mean(np.clip(gt_img[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_condition = np.mean(np.clip(condition_img[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_noise2noise = np.mean(np.clip(noise2noise_img[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_noise2score = np.mean(np.clip(noise2score_img[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "\n",
    "    print('mean: gt:', mean_gt, ' condition:', mean_condition, ' noise2noise:', mean_noise2noise, ' noise2score:', mean_noise2score)\n",
    "\n",
    "    delta = mean_noise2noise - mean_noise2score\n",
    "\n",
    "    noise2score_img += delta\n",
    "    affine = nb.load(noise2score_file).affine\n",
    "    nb.save(nb.Nifti1Image(noise2score_img, affine), os.path.join('/host/d/projects/denoising/models/noise2score_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch100/pred_img_postprocess.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00214841 0000455418 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  5.262398913779201 0.4221762951842335 0.169640893638134\n",
      "noise2noise:  3.525687212503505 0.7242976452876638 0.11386577293276787\n",
      "noise2score:  4.290063714582379 0.7369527171209164 0.10267281696200371\n",
      "unsupervised:  4.9903326804242365 0.411119739757146 0.1551213553547859\n",
      "unsupervised_avg10:  2.345594789700026 0.7413237973703097 0.05306417040526867\n",
      "unsupervised_avg20:  2.0924936735485624 0.7937009671853048 0.03779044929891825\n",
      "distilled:  2.3680856896644302 0.8229981331263108 0.04444339271634817\n",
      "00105734 0000455323 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  7.337162982738693 0.34091548341918654 0.1902868089079857\n",
      "noise2noise:  4.478628805478383 0.671663308098533 0.10457669630646706\n",
      "noise2score:  6.1178708444202705 0.6586241319045534 0.09823435842990876\n",
      "unsupervised:  5.638568477761703 0.3934165410481221 0.1477609768509865\n",
      "unsupervised_avg10:  2.898833411592416 0.693219368043294 0.05218272276222706\n",
      "unsupervised_avg20:  2.652895050040428 0.7398319532623138 0.0402244234085083\n",
      "distilled:  3.112148819767427 0.7645694054606422 0.052385647967457774\n",
      "00214867 0000455521 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  6.449216671289224 0.43833770875368516 0.14771801799535753\n",
      "noise2noise:  4.073729407076243 0.6842181882919007 0.10579396620392799\n",
      "noise2score:  4.3967739440624465 0.5928470178121941 0.10112148106098175\n",
      "unsupervised:  5.626173344633871 0.3966805309117449 0.11538859933614731\n",
      "unsupervised_avg10:  3.089030906352456 0.6547440597802635 0.04790429316461086\n",
      "unsupervised_avg20:  2.879995371009153 0.690705828473086 0.053775690495967865\n",
      "distilled:  3.1644186074061142 0.7127882915117228 0.08634012371301651\n",
      "00214877 0000455524 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  5.7362697872554325 0.48663729230443253 0.11353586018085479\n",
      "noise2noise:  4.312554823322816 0.7274260037704693 0.08346265256404876\n",
      "noise2score:  5.000324785950775 0.6183594904106003 0.09239318192005158\n",
      "unsupervised:  5.387702448186999 0.43997420642953877 0.09745911374688149\n",
      "unsupervised_avg10:  2.9840715323108498 0.7017368943512821 0.04138797886669636\n",
      "unsupervised_avg20:  2.7830959405092623 0.7354337423952813 0.044760881513357165\n",
      "distilled:  3.264968426601279 0.7471520212243326 0.0697492779046297\n",
      "00214836 0000455414 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  6.302141485415822 0.31540359896536024 0.17340115189552308\n",
      "noise2noise:  3.8000416551168126 0.6468763509809705 0.10874634549021721\n",
      "noise2score:  5.957889039200134 0.6896428217636218 0.099685787409544\n",
      "unsupervised:  5.433432281886552 0.3445653541468332 0.15961723476648332\n",
      "unsupervised_avg10:  2.57022656700744 0.699917350125618 0.06584141589701176\n",
      "unsupervised_avg20:  2.2992253440101726 0.7588255431186872 0.04730062983930111\n",
      "distilled:  2.5223056799042087 0.7889757005262433 0.048556502237915994\n",
      "00154137 0000455529 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  6.740815307645066 0.4662036955296754 0.18378310322761535\n",
      "noise2noise:  4.591293460149514 0.7059293362012738 0.11542524695396424\n",
      "noise2score:  5.056737213294329 0.6411900464865578 0.10382305234670638\n",
      "unsupervised:  5.882757512033655 0.44540368493536137 0.1441258379817009\n",
      "unsupervised_avg10:  3.3172713144557076 0.6941392350468569 0.05699150577187538\n",
      "unsupervised_avg20:  3.104812536215377 0.7283196555556516 0.05506131172180176\n",
      "distilled:  3.5622203417979024 0.7431742538592718 0.07380322754383087\n",
      "00214901 0000455577 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  7.037874286129568 0.40387359484336555 0.15406421899795533\n",
      "noise2noise:  4.540646071410608 0.6699532952007675 0.10724699154496192\n",
      "noise2score:  5.612518360730637 0.5689844952160273 0.09976495116949081\n",
      "unsupervised:  5.769975125671374 0.402071689122389 0.11928316965699196\n",
      "unsupervised_avg10:  3.211769534826219 0.6657406930537678 0.053949148803949354\n",
      "unsupervised_avg20:  2.998814129746789 0.7018635056480674 0.054713860526680945\n",
      "distilled:  3.4300022151320193 0.7160717477566682 0.0814143280684948\n",
      "00174234 0000455725 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  5.7271966580852975 0.4863898889031988 0.14791991993784903\n",
      "noise2noise:  4.089859309456272 0.7069581997422644 0.11650239422917366\n",
      "noise2score:  5.0444852600952865 0.6451639920007534 0.09959097385406494\n",
      "unsupervised:  5.30434583513132 0.43116898896037287 0.11980743423104286\n",
      "unsupervised_avg10:  2.9505014617718 0.6861991232711907 0.05225233376026153\n",
      "unsupervised_avg20:  2.750919458524444 0.7209808797779328 0.05643876634538174\n",
      "distilled:  3.1648701877645924 0.7371963171153207 0.07874048262834549\n",
      "00035838 0000455369 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  5.4710435491401235 0.4642504424325633 0.11960821405053139\n",
      "noise2noise:  3.6857408819212507 0.7301222998570858 0.07530868738889694\n",
      "noise2score:  4.546948782283016 0.664177007762223 0.07898313246667385\n",
      "unsupervised:  5.303981569723456 0.41337862568642464 0.10796573013067245\n",
      "unsupervised_avg10:  2.72879409970319 0.706273286696866 0.03611165296286344\n",
      "unsupervised_avg20:  2.50180199118654 0.7474379879449828 0.03598284777253866\n",
      "distilled:  2.790777956427712 0.768935410880629 0.05435664430260658\n",
      "00019591 0000455445 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  6.3761041569778785 0.3960709822064643 0.14154353231191635\n",
      "noise2noise:  3.770160209649059 0.7009809220511145 0.08679238811135292\n",
      "noise2score:  5.040073857783898 0.6340343228161661 0.0938999493420124\n",
      "unsupervised:  5.61848983193352 0.3745259893096739 0.12470711246132851\n",
      "unsupervised_avg10:  2.91622943419085 0.6744641895654975 0.04212107136845589\n",
      "unsupervised_avg20:  2.68484798551756 0.716944047662409 0.03887475438416004\n",
      "distilled:  2.820167806595066 0.7434042614378983 0.057346446886658665\n",
      "00214792 0000455334 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  5.528428690838294 0.44698273322028853 0.15925747096538545\n",
      "noise2noise:  4.423117476576574 0.7188849041962676 0.09314140066504478\n",
      "noise2score:  5.769954483043859 0.6902418043683956 0.0883574977517128\n",
      "unsupervised:  5.10108442480583 0.43235435349548773 0.13958775296807288\n",
      "unsupervised_avg10:  2.725625536397757 0.7142769517049814 0.04318534605205059\n",
      "unsupervised_avg20:  2.515384217154653 0.753268075007619 0.039430779069662095\n",
      "distilled:  3.0142630638467494 0.7704974545523156 0.05944735020399094\n",
      "00010461 0000455845 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  6.43744125553988 0.3944770089938624 0.10271216928958893\n",
      "noise2noise:  4.260620684628248 0.619341329238842 0.06616340845823288\n",
      "noise2score:  4.915634245349073 0.6174067488472745 0.0692403021454811\n",
      "unsupervised:  5.541322192709727 0.39606818235464003 0.08904828473925591\n",
      "unsupervised_avg10:  2.9256924760669905 0.6861269271211965 0.034525436833500865\n",
      "unsupervised_avg20:  2.697780061406393 0.7296044185697201 0.031088127419352532\n",
      "distilled:  2.980527153527597 0.7512374657863401 0.045783331990242\n",
      "00214931 0000455660 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  6.508538380256226 0.32134476459314193 0.17635746985673906\n",
      "noise2noise:  3.4044918180185846 0.7056243069244794 0.10311220765113831\n",
      "noise2score:  3.5870656417247644 0.6934142061184736 0.08828916475176811\n",
      "unsupervised:  5.366648104054925 0.3512542308911763 0.1467745354771614\n",
      "unsupervised_avg10:  2.574799935039552 0.6878595724931961 0.05704256065189838\n",
      "unsupervised_avg20:  2.3116665792617197 0.7441368361124165 0.043294581547379495\n",
      "distilled:  2.584939484015449 0.779723341246033 0.05009909525513649\n",
      "00010436 0000034479 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  5.9568505300854815 0.4086262005570566 0.17138932764530182\n",
      "noise2noise:  4.097391327584733 0.7393736697374342 0.09502151995897293\n",
      "noise2score:  4.616112542248397 0.7051886322479216 0.09114059016108512\n",
      "unsupervised:  5.109812915271164 0.43074896441464294 0.13968774646520615\n",
      "unsupervised_avg10:  2.5584328080868826 0.7269703129431115 0.04333424285054207\n",
      "unsupervised_avg20:  2.3211315989122143 0.7732305657222409 0.034858427792787555\n",
      "distilled:  2.848717448156261 0.8008693632307804 0.049040183201432226\n",
      "00214878 0000455518 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  6.963043899480919 0.42771182220670567 0.15204572051763535\n",
      "noise2noise:  4.848371768265643 0.6737854692225875 0.09214182779192924\n",
      "noise2score:  4.774371300592005 0.6515791256929359 0.08546584919095039\n",
      "unsupervised:  5.566548000584895 0.4599657077429871 0.10055831611156464\n",
      "unsupervised_avg10:  3.007998470639946 0.7185013464718375 0.03152061883360147\n",
      "unsupervised_avg20:  2.7863730610860045 0.7560565199469892 0.030113748610019683\n",
      "distilled:  3.323383327067639 0.7750551585295804 0.04775072544813156\n",
      "00014689 0000455416 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "condition:  6.705985863370074 0.36483990885290574 0.1620174217224121\n",
      "noise2noise:  4.018281877045435 0.661798083360146 0.11736852869391441\n",
      "noise2score:  3.967194343812374 0.6771942214318265 0.09344840988516807\n",
      "unsupervised:  5.520103383401314 0.3969938397213478 0.12842388167977334\n",
      "unsupervised_avg10:  2.7903551698953093 0.7026291141109455 0.046864577904343604\n",
      "unsupervised_avg20:  2.5421091768426827 0.75267153903162 0.0371755537763238\n",
      "distilled:  2.8750502874600783 0.7806706877073678 0.04880617097020149\n"
     ]
    }
   ],
   "source": [
    "## metric calculations\n",
    "results = []\n",
    "for i in range(0,n.shape[0]):\n",
    "    patient_id = patient_id_list[n[i]]\n",
    "    patient_subid = patient_subid_list[n[i]]\n",
    "    random_n = random_num_list[n[i]]\n",
    "    print(patient_id, patient_subid, random_n)\n",
    "\n",
    "    # reference image\n",
    "    gt_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/gt_img.nii.gz')\n",
    "    gt_img = nb.load(gt_file).get_fdata()\n",
    "\n",
    "    # noisy image\n",
    "    condition_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/condition_img.nii.gz')\n",
    "    condition_img = nb.load(condition_file).get_fdata()\n",
    "\n",
    "    # noise2noise\n",
    "    noise2noise_file = os.path.join('/host/d/projects/denoising/models/noise2noise_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch78/pred_img.nii.gz')\n",
    "    noise2noise_img = nb.load(noise2noise_file).get_fdata() \n",
    "\n",
    "    # noise2score\n",
    "    noise2score_file = os.path.join('/host/d/projects/denoising/models/noise2score_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch100/pred_img_postprocess.nii.gz')\n",
    "    noise2score_img = nb.load(noise2score_file).get_fdata()\n",
    "\n",
    "    # supervised method\n",
    "    # supervised_file = os.path.join('/mnt/camca_NAS/denoising/models/supervised_poisson/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch58_1/pred_img.nii.gz')\n",
    "    # supervised_img = nb.load(supervised_file).get_fdata() \n",
    "\n",
    "    # our method (unsupervised), 1 inference\n",
    "    unsupervised_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/pred_img.nii.gz')\n",
    "    unsupervised_img = nb.load(unsupervised_file).get_fdata()\n",
    "\n",
    "    # our method (unsupervised), 10 inference\n",
    "    unsupervised_avg10_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61avg/pred_img_scans10.nii.gz')\n",
    "    unsupervised_avg10_img = nb.load(unsupervised_avg10_file).get_fdata()\n",
    "\n",
    "    # our method (unsupervised), beta = 0, 20 inference\n",
    "    unsupervised_avg20_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61avg/pred_img_scans20.nii.gz')\n",
    "    unsupervised_avg20_img = nb.load(unsupervised_avg20_file).get_fdata()\n",
    "\n",
    "    # our method (distilled DDPM)\n",
    "    distilled_file = os.path.join('/host/d/projects/denoising/models/distill_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch164_1/pred_img.nii.gz')\n",
    "    distilled_img = nb.load(distilled_file).get_fdata()\n",
    "\n",
    "    \n",
    "\n",
    "    ## calculate metrics\n",
    "    # MAE\n",
    "    vmin = 0\n",
    "    vmax = 100\n",
    "    mae_condition, mae_condition_std = calc_mae_with_ref_window(condition_img, gt_img, vmin, vmax)\n",
    "    mae_noise2noise, mae_noise2noise_std = calc_mae_with_ref_window(noise2noise_img, gt_img, vmin, vmax)\n",
    "    mae_noise2score, mae_noise2score_std = calc_mae_with_ref_window(noise2score_img, gt_img, vmin, vmax)\n",
    "    # mae_supervised, mae_supervised_std = calc_mae_with_ref_window(supervised_img, gt_img, vmin, vmax)\n",
    "    mae_unsupervised, mae_unsupervised_std = calc_mae_with_ref_window(unsupervised_img, gt_img, vmin, vmax)\n",
    "    mae_unsupervised_avg10, mae_unsupervised_avg10_std = calc_mae_with_ref_window(unsupervised_avg10_img, gt_img, vmin, vmax)\n",
    "    mae_unsupervised_avg20, mae_unsupervised_avg20_std = calc_mae_with_ref_window(unsupervised_avg20_img, gt_img, vmin, vmax)\n",
    "    mae_distilled, mae_distilled_std = calc_mae_with_ref_window(distilled_img, gt_img, vmin, vmax)\n",
    "   \n",
    "    \n",
    "    # # SSIM\n",
    "    ssim_condition, ssim_condition_std = calc_ssim_with_ref_window(condition_img, gt_img, vmin, vmax)\n",
    "    ssim_noise2noise, ssim_noise2noise_std = calc_ssim_with_ref_window(noise2noise_img, gt_img, vmin, vmax)\n",
    "    ssim_noise2score, ssim_noise2score_std = calc_ssim_with_ref_window(noise2score_img, gt_img, vmin, vmax)\n",
    "    # ssim_supervised, ssim_supervised_std = calc_ssim_with_ref_window(supervised_img, gt_img, vmin, vmax)\n",
    "    ssim_unsupervised, ssim_unsupervised_std = calc_ssim_with_ref_window(unsupervised_img, gt_img, vmin, vmax)\n",
    "    ssim_unsupervised_avg10, ssim_unsupervised_avg10_std = calc_ssim_with_ref_window(unsupervised_avg10_img, gt_img, vmin, vmax)\n",
    "    ssim_unsupervised_avg20, ssim_unsupervised_avg20_std = calc_ssim_with_ref_window(unsupervised_avg20_img, gt_img, vmin, vmax)\n",
    "    ssim_distilled, ssim_distilled_std = calc_ssim_with_ref_window(distilled_img, gt_img, vmin, vmax)\n",
    "    \n",
    "\n",
    "    # # lpips\n",
    "    lpips_condition, _ = calc_lpips(condition_img, gt_img, vmin, vmax)\n",
    "    lpips_noise2noise, _ = calc_lpips(noise2noise_img, gt_img, vmin, vmax)\n",
    "    lpips_noise2score, _ = calc_lpips(noise2score_img, gt_img, vmin, vmax)\n",
    "    # lpips_supervised, _ = calc_lpips(supervised_img, gt_img, vmin, vmax)\n",
    "    lpips_unsupervised, _ = calc_lpips(unsupervised_img, gt_img, vmin, vmax)\n",
    "    lpips_unsupervised_avg10, _ = calc_lpips(unsupervised_avg10_img, gt_img, vmin, vmax)\n",
    "    lpips_unsupervised_avg20, _ = calc_lpips(unsupervised_avg20_img, gt_img, vmin, vmax)\n",
    "    lpips_distilled, _ = calc_lpips(distilled_img, gt_img, vmin, vmax)\n",
    "\n",
    "    print('condition: ', mae_condition, ssim_condition, lpips_condition)\n",
    "    print('noise2noise: ', mae_noise2noise, ssim_noise2noise, lpips_noise2noise)\n",
    "    print('noise2score: ', mae_noise2score, ssim_noise2score, lpips_noise2score)\n",
    "    # print('supervised: ', mae_supervised, ssim_supervised, lpips_supervised)\n",
    "    print('unsupervised: ', mae_unsupervised, ssim_unsupervised, lpips_unsupervised)\n",
    "    print('unsupervised_avg10: ', mae_unsupervised_avg10, ssim_unsupervised_avg10, lpips_unsupervised_avg10)\n",
    "    print('unsupervised_avg20: ', mae_unsupervised_avg20, ssim_unsupervised_avg20, lpips_unsupervised_avg20)\n",
    "    print('distilled: ', mae_distilled, ssim_distilled, lpips_distilled)\n",
    "\n",
    "    results.append([patient_id, patient_subid, random_n,\n",
    "                    mae_condition,mae_noise2noise,mae_noise2score,mae_unsupervised,mae_unsupervised_avg10,mae_unsupervised_avg20,mae_distilled,\n",
    "                    ssim_condition,ssim_noise2noise,ssim_noise2score,ssim_unsupervised,ssim_unsupervised_avg10,ssim_unsupervised_avg20,ssim_distilled,\n",
    "                    lpips_condition,lpips_noise2noise,lpips_noise2score,lpips_unsupervised,lpips_unsupervised_avg10,lpips_unsupervised_avg20,lpips_distilled\n",
    "                    ])\n",
    "\n",
    "    dd = pd.DataFrame(results, columns = ['patient_id','patient_subid','random_n',\n",
    "                                             'mae_condition','mae_noise2noise','mae_noise2score','mae_unsupervised','mae_unsupervised_avg10','mae_unsupervised_avg20','mae_distilled',\n",
    "                                             'ssim_condition','ssim_noise2noise','ssim_noise2score','ssim_unsupervised','ssim_unsupervised_avg10','ssim_unsupervised_avg20','ssim_distilled',\n",
    "                                             'lpips_condition','lpips_noise2noise','lpips_noise2score','lpips_unsupervised','lpips_unsupervised_avg10','lpips_unsupervised_avg20','lpips_distilled'\n",
    "                                             ])\n",
    "    dd.to_excel('/host/d/projects/denoising/results/brainCT_results.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00214841 0000455418 0\n",
      "mean: gt: 21.754015723196854  condition: 21.89032840789795  noise2noise: 19.03606891879365  noise2score: 15.506743506469727\n",
      "00105734 0000455323 0\n",
      "mean: gt: 18.486178371301225  condition: 18.97817980621338  noise2noise: 14.566930114135912  noise2score: 14.303804944152832\n",
      "00214867 0000455521 0\n",
      "mean: gt: 32.106963860106056  condition: 32.13290597808838  noise2noise: 30.043341023751566  noise2score: 25.375424999389647\n",
      "00214877 0000455524 0\n",
      "mean: gt: 31.27295554296927  condition: 31.359965008850097  noise2noise: 28.43605288487233  noise2score: 25.188118078308104\n",
      "00214836 0000455414 0\n",
      "mean: gt: 15.307165243522553  condition: 15.87778719329834  noise2noise: 11.581240975344958  noise2score: 9.508770465698243\n",
      "00154137 0000455529 0\n",
      "mean: gt: 32.78842260734797  condition: 32.843813273010255  noise2noise: 29.875630303859552  noise2score: 26.09790611022949\n",
      "00214901 0000455577 0\n",
      "mean: gt: 25.038895094831215  condition: 25.23733533203125  noise2noise: 21.86812425697546  noise2score: 20.032635048828126\n",
      "00174234 0000455725 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# noise2score\u001b[39;00m\n\u001b[1;32m     23\u001b[0m noise2score_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/host/d/projects/denoising/models/noise2score_brainCT/pred_images\u001b[39m\u001b[38;5;124m'\u001b[39m, patient_id, patient_subid,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(random_n), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch100/pred_img.nii.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m noise2score_img \u001b[38;5;241m=\u001b[39m \u001b[43mnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise2score_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m## calculate main\u001b[39;00m\n\u001b[1;32m     29\u001b[0m x,y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nibabel/dataobj_images.py:373\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[0;34m(self, caching, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nibabel/arrayproxy.py:439\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nibabel/arrayproxy.py:406\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    404\u001b[0m     scl_inter \u001b[38;5;241m=\u001b[39m scl_inter\u001b[38;5;241m.\u001b[39mastype(use_dtype)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_unscaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslicer\u001b[49m\u001b[43m)\u001b[49m, scl_slope, scl_inter)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m scaled\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mpromote_types(scaled\u001b[38;5;241m.\u001b[39mdtype, dtype), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nibabel/arrayproxy.py:376\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical_slicers(slicer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m==\u001b[39m canonical_slicers(\n\u001b[1;32m    373\u001b[0m     (), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    374\u001b[0m ):\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fileslice(\n\u001b[1;32m    386\u001b[0m         fileobj,\n\u001b[1;32m    387\u001b[0m         slicer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m         lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock,\n\u001b[1;32m    393\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nibabel/volumeutils.py:465\u001b[0m, in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(infile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    464\u001b[0m     data_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(n_bytes)\n\u001b[0;32m--> 465\u001b[0m     n_read \u001b[38;5;241m=\u001b[39m \u001b[43minfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     needs_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/_compression.py:69\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[1;32m     68\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(byte_view))\n\u001b[0;32m---> 69\u001b[0m     byte_view[:\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calculate mean\n",
    "## metric calculations\n",
    "results = []\n",
    "for i in range(0,n.shape[0]):\n",
    "    patient_id = patient_id_list[n[i]]\n",
    "    patient_subid = patient_subid_list[n[i]]\n",
    "    random_n = random_num_list[n[i]]\n",
    "    print(patient_id, patient_subid, random_n)\n",
    "\n",
    "    # reference image\n",
    "    gt_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/gt_img.nii.gz')\n",
    "    gt_img = nb.load(gt_file).get_fdata()\n",
    "\n",
    "    # noisy image\n",
    "    condition_file = os.path.join('/host/d/projects/denoising/models/unsupervised_gaussian_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/condition_img.nii.gz')\n",
    "    condition_img = nb.load(condition_file).get_fdata()\n",
    "\n",
    "    # noise2noise\n",
    "    noise2noise_file = os.path.join('/host/d/projects/denoising/models/noise2noise_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch78/pred_img.nii.gz')\n",
    "    noise2noise_img = nb.load(noise2noise_file).get_fdata() \n",
    "\n",
    "    # noise2score\n",
    "    noise2score_file = os.path.join('/host/d/projects/denoising/models/noise2score_brainCT/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch100/pred_img.nii.gz')\n",
    "    noise2score_img = nb.load(noise2score_file).get_fdata()\n",
    "\n",
    "\n",
    "    ## calculate main\n",
    "    x,y = 256,256\n",
    "    mean_gt = np.mean(np.clip(gt_img[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_condition = np.mean(np.clip(condition_img[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_noise2noise = np.mean(np.clip(noise2noise_img[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_noise2score = np.mean(np.clip(noise2score_img[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "\n",
    "    print('mean: gt:', mean_gt, ' condition:', mean_condition, ' noise2noise:', mean_noise2noise, ' noise2score:', mean_noise2score)\n",
    "    \n",
    "    results.append([patient_id, patient_subid, random_n,\n",
    "                    mean_gt, mean_condition, mean_noise2noise, mean_noise2score\n",
    "                    ])\n",
    "    dd = pd.DataFrame(results, columns = ['patient_id','patient_subid','random_n',\n",
    "                                             'mean_gt','mean_condition','mean_noise2noise','mean_noise2score'\n",
    "                                             ])\n",
    "\n",
    "    dd.to_excel('/host/d/projects/denoising/results/brainCT_mean_results.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
